{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "from nltk import download\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import RegexpParser\n",
    "from nltk import pos_tag\n",
    "\n",
    "import string\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove non ASCII characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non ASCII characters\n",
    "def strip_non_ascii(data_str):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in data_str if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)\n",
    "\n",
    "# setup pyspark udf function\n",
    "strip_non_ascii_udf = udf(strip_non_ascii, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the user defined function of removing non ASCII characters\n",
    "df = df.withColumn('text_non_asci',strip_non_ascii_udf(df['review_body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify abbreviations\n",
    "def fix_abbreviation(data_str):\n",
    "    data_str = data_str.lower()\n",
    "    data_str = re.sub(r'\\bthats\\b', 'that is', data_str)\n",
    "    data_str = re.sub(r'\\bive\\b', 'i have', data_str)\n",
    "    data_str = re.sub(r'\\bim\\b', 'i am', data_str)\n",
    "    data_str = re.sub(r'\\bya\\b', 'yeah', data_str)\n",
    "    data_str = re.sub(r'\\bcant\\b', 'can not', data_str)\n",
    "    data_str = re.sub(r'\\bdont\\b', 'do not', data_str)\n",
    "    data_str = re.sub(r'\\bwont\\b', 'will not', data_str)\n",
    "    data_str = re.sub(r'\\bid\\b', 'i would', data_str)\n",
    "    data_str = re.sub(r'wtf', 'what the fuck', data_str)\n",
    "    data_str = re.sub(r'\\bwth\\b', 'what the hell', data_str)\n",
    "    data_str = re.sub(r'\\br\\b', 'are', data_str)\n",
    "    data_str = re.sub(r'\\bu\\b', 'you', data_str)\n",
    "    data_str = re.sub(r'\\bk\\b', 'OK', data_str)\n",
    "    data_str = re.sub(r'\\bsux\\b', 'sucks', data_str)\n",
    "    data_str = re.sub(r'\\bno+\\b', 'no', data_str)\n",
    "    data_str = re.sub(r'\\bcoo+\\b', 'cool', data_str)\n",
    "    data_str = re.sub(r'rt\\b', '', data_str)\n",
    "    data_str = data_str.strip()\n",
    "    return data_str\n",
    "\n",
    "# setup pyspark udf function\n",
    "fix_abbreviation_udf = udf(fix_abbreviation, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the user defined function of modifying abbreviations\n",
    "df = df.withColumn('text_fixed_abbrev',fix_abbreviation_udf(df['text_non_asci']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove hyperlinks, puncuations, numbers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hyperlinks, puncuations, numbers, etc.\n",
    "def remove_features(data_str):\n",
    "    # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    mention_re = re.compile('@(\\w+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    html_re = re.compile(\"<br />\")\n",
    "    # convert to lowercase\n",
    "    data_str = data_str.lower()\n",
    "    # remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "    # remove @mentions\n",
    "    data_str = mention_re.sub(' ', data_str)\n",
    "    # remove puncuation\n",
    "    data_str = punc_re.sub(' ', data_str)\n",
    "    # remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "    # remove html symbol\n",
    "    data_str = html_re.sub(' ', data_str)   \n",
    "    # remove non a-z 0-9 characters and words shorter than 1 characters\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    for word in data_str.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word) and len(word) > 1:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word) and len(word) > 1:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            else:\n",
    "                cleaned_str += ' '\n",
    "        list_pos += 1\n",
    "    # remove unwanted space, *.split() will automatically split on\n",
    "    # whitespace and discard duplicates, the \" \".join() joins the\n",
    "    # resulting list into one string.\n",
    "    return \" \".join(cleaned_str.split())\n",
    "\n",
    "# setup pyspark udf function\n",
    "remove_features_udf = udf(remove_features, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the user defined function of removing hyperlinks, punctuations, numbers, etc.\n",
    "df = df.withColumn('text_feature_removed',remove_features_udf(df['text_fixed_abbrev']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group together the different inflected forms of a word "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convert past tense and future tense into simple present tense\n",
    "- convert plural form into singular form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the empty non-type values\n",
    "df = df.where(df.text_feature_removed.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group together the different inflected forms of a word\n",
    "def lemmatize(data_str):\n",
    "    # expects a string\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    text = data_str.split()\n",
    "    tagged_words = pos_tag(text)\n",
    "    for word in tagged_words:\n",
    "        if 'v' in word[1].lower():\n",
    "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
    "        else:\n",
    "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
    "        if list_pos == 0:\n",
    "            cleaned_str = lemma\n",
    "        else:\n",
    "            cleaned_str = cleaned_str + ' ' + lemma\n",
    "        list_pos += 1\n",
    "    return cleaned_str\n",
    "\n",
    "# setup pyspark udf function\n",
    "lemmatize_udf = udf(lemmatize, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the user defined function of lemmatizing words with different tenses and forms\n",
    "lemm_df = df.withColumn(\"lemm_text\", lemmatize_udf(df[\"text_feature_removed\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mark up a word in a text as corresponding to a particular part of speech, based on both its definition and its contextâ€Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identify different part of the speech\n",
    "- Combine patterns such as \"noun + noun\" and \"adjective + noun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the empty non-type values\n",
    "lemm_df = lemm_df.where(lemm_df.lemm_text.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_and_remove(data_str):\n",
    "    cleaned_str = ' '\n",
    "    # noun tags\n",
    "    nn_tags = ['NN', 'NNP','NNS','NNP','NNPS']\n",
    "    # adjectives\n",
    "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
    "    # verbs\n",
    "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
    "\n",
    "    # break string into 'words'\n",
    "    text = data_str.split()\n",
    "    \n",
    "    text_notype = []\n",
    "    for w in text:\n",
    "        if w is None:\n",
    "            continue\n",
    "        else:\n",
    "            text_notype.append(w)\n",
    "\n",
    "    # tag the text and keep only those with the right tags\n",
    "    tagged_text = pos_tag(text_notype)\n",
    "    \n",
    "    for i in range(len(tagged_text)):\n",
    "        if tagged_text[i][1] in nltk_tags:\n",
    "            if i < len(tagged_text)-1:\n",
    "                if (tagged_text[i][1] in nn_tags) and (tagged_text[i+1][1] in nn_tags):\n",
    "                    cleaned_str += tagged_text[i][0] + '_'\n",
    "                elif (tagged_text[i][1] in jj_tags) and (tagged_text[i+1][1] in nn_tags):\n",
    "                    cleaned_str += tagged_text[i][0] + '_'  \n",
    "                else:\n",
    "                    cleaned_str += tagged_text[i][0] + ' '\n",
    "#    for tagged_word in tagged_text:\n",
    "#        if tagged_word[1] in nltk_tags:\n",
    "#            cleaned_str += tagged_word[0] + ' '\n",
    "            \n",
    "\n",
    "    return cleaned_str\n",
    "\n",
    "# setup pyspark udf function\n",
    "tag_and_remove_udf = udf(tag_and_remove, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the user defined function of tagging by part of speech\n",
    "tagged_df = lemm_df.withColumn(\"tag_text\", tag_and_remove_udf(lemm_df.lemm_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the empty non-type values\n",
    "tagged_df = tagged_df.where(tagged_df.tag_text.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('br')\n",
    "stop_words.append('would')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "def remove_stops(data_str):\n",
    "    # expects a string\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    text = data_str.split()\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            # rebuild cleaned_str\n",
    "            if list_pos == 0:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            list_pos += 1\n",
    "    return cleaned_str\n",
    "\n",
    "# setup pyspark udf function\n",
    "remove_stops_udf = udf(remove_stops, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the user defined function of removing stop words\n",
    "stop_df= tagged_df.withColumn(\"stop_text\", remove_stops_udf(tagged_df[\"tag_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the reviews into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup pyspark udf function\n",
    "tokenize_udf = udf(word_tokenize, ArrayType(StringType()))\n",
    "\n",
    "token_df = stop_df.withColumn(\"token_text\", tokenize_udf(stop_df[\"stop_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the reviews from 2015 as testing dataset and others as training datasets\n",
    "#### Group reviews by ratings and years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the empty non-type values\n",
    "token_df = token_df.where(token_df.token_text.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine_train = token_df.where(\"year != 2015\").groupby('star_rating').agg(collect_list('token_text').alias(\"review_clean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine_test = token_df.where(\"year = 2015\").groupby('star_rating').agg(collect_list('token_text').alias(\"review_clean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine_train = df_combine_train.where(df_combine_train.review_clean.isNotNull())\n",
    "df_combine_test = df_combine_test.where(df_combine_test.review_clean.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the nested lists\n",
    "def flatten_nested_list(nested_list):\n",
    "    flatten_list = list(itertools.chain.from_iterable(nested_list))\n",
    "    return flatten_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_udf = udf(flatten_nested_list, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine_train = df_combine_train.withColumn('review_cleaned', flatten_udf(df_combine_train.review_clean))\n",
    "df_combine_test = df_combine_test.withColumn('review_cleaned', flatten_udf(df_combine_test.review_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = df_combine_train.sort(\"star_rating\",ascending=True).select('star_rating','review_cleaned').collect()\n",
    "texts_test = df_combine_test.sort(\"star_rating\",ascending=True).select('star_rating','review_cleaned').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the documents for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_train = []\n",
    "for i in range(len(texts_train)):\n",
    "    documents_train.append(texts_train[i].review_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_test = []\n",
    "for i in range(len(texts_test)):\n",
    "    documents_test.append(texts_test[i].review_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering out the most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train = {}\n",
    "for i in documents_train:\n",
    "    for j in i:\n",
    "        if j in dict_train.keys():\n",
    "            dict_train[j] += 1\n",
    "        else:\n",
    "            dict_train[j] = dict_train.get(j, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_test = {}\n",
    "for i in documents_test:\n",
    "    for j in i:\n",
    "        if j in dict_test.keys():\n",
    "            dict_test[j] += 1\n",
    "        else:\n",
    "            dict_test[j] = dict_test.get(j, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frequent_words_train=[]\n",
    "for k, v in dict_train.items():\n",
    "    if v > 90:\n",
    "        n_frequent_words_train.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frequent_words_test=[]\n",
    "for k, v in dict_test.items():\n",
    "    if v > 75:\n",
    "        n_frequent_words_test.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_train_filter = []\n",
    "for l in documents_train:\n",
    "    new_l = l\n",
    "    for w in new_l:\n",
    "        if w in n_frequent_words_train:\n",
    "            new_l.remove(w)\n",
    "    documents_train_filter.append(new_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_test_filter = []\n",
    "for l in documents_test:\n",
    "    new_l = l\n",
    "    for w in new_l:\n",
    "        if w in n_frequent_words_test:\n",
    "            new_l.remove(w)\n",
    "    documents_test_filter.append(new_l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
